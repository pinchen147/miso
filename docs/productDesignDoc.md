Great — I’ll fully revise the product design document to incorporate:

1. A new feature that allows the AI to visually assess available cookware (e.g., pans, pots, tools) and adapt recipe steps using RAG, just as it does for ingredient substitutions.
2. A restructured system architecture using Supabase as the backend, aligned to the schema you've provided for ingredients, recipes, steps, and profiles.

I'll reorganize the relevant architecture, RAG logic, and database sections accordingly and get the updated design doc ready for your review.


# iOS AI Cooking Assistant – Product Design Document (Revised)

## Overview

Cooking at home can be challenging – especially when recipes call for tools you don’t have or ingredients you can’t find. The **iOS AI Cooking Assistant** is a hands-free, intelligent cooking guide that adapts to *your* kitchen. It uses the iPhone’s camera, audio, and AI to coach you through recipes step by step, all while considering the equipment and ingredients you actually have available. By leveraging real-time computer vision, voice interaction, and retrieval-augmented generation (RAG) on a rich cooking knowledge base, the assistant provides context-aware instructions, substitutions, and adjustments on the fly. The result is an interactive “AI sous-chef” that makes cooking more accessible, personalized, and error-resistant for users of all skill levels.

This app is **iOS-first**, taking advantage of Apple’s AR and audio frameworks for a seamless experience on iPhone. The core functionality is powered by cloud-based AI (Google’s Gemini 2.5 multimodal model) running at \~1 frame per second (FPS) for visual analysis, combined with a Supabase-backed database for recipe content and embeddings. The system design emphasizes:

* **Fully hands-free operation:** No need to touch the screen with messy hands – control is via voice and vision.
* **Step-by-step audio guidance:** Clear spoken instructions that adjust to your pace and reference what you’ve done so far.
* **Visual understanding and feedback:** The camera watches your cooking, identifying ingredients and actions to give timely feedback or corrections.
* **Adaptive guidance based on context:** The assistant adapts instructions if you’re using different equipment or ingredients, retrieving expert advice via RAG to suggest suitable alternatives.
* **Recipe flexibility:** Cook from a built-in library of classic recipes or import your own (text or PDF). The app parses any recipe into interactive guided steps.
* **Personalization and learning:** The system remembers your preferences (e.g. dietary needs, available appliances) to tailor recipe suggestions and substitution recommendations over time.
* **Privacy and simplicity:** Camera feed is analyzed in real time but not stored; user data (recipes, profiles) is securely managed via Supabase. The app can work in a basic offline mode (limited to pre-downloaded recipes and no vision feedback) if needed, while full features require internet for AI services.
* **Monetization:** A subscription model unlocks premium content (specialty recipes, advanced AI features like deeper substitutions and appliance-specific recipes), while the free tier offers core functionality and a base recipe set without ads.

By combining cutting-edge AI with practical cooking knowledge, the iOS AI Cooking Assistant aims to replicate the guidance of an expert chef who knows *you* and your kitchen, making home cooking easier, smarter, and more enjoyable.

## Feature Overview

The AI Cooking Assistant’s core features and capabilities include:

* **Real-Time Ingredient Recognition:** Using the iPhone’s front camera, the app identifies ingredients and food items in view. It highlights and labels ingredients (e.g. detecting a tomato, onion, or spice jar) on-screen in real time. This helps verify that you have the right ingredients ready and lets the assistant track ingredient usage. For example, if a recipe calls for garlic and ginger, the camera can confirm these are present and even point to them so you can grab them easily. Ingredient recognition runs at about 1 FPS using a powerful cloud vision model, ensuring up-to-date identification even for a wide variety of produce and pantry items.

* **Step-by-Step Audio Guidance:** The assistant provides *continuous, spoken* recipe instructions, enabling you to cook without ever glancing at a cookbook or phone screen. Each step of the chosen recipe is narrated via high-quality text-to-speech. Instructions are given in a clear, patient voice (with an optional choice of voice personas) and are tailored to what’s happening: e.g., “Now crack two eggs into the bowl and whisk them lightly.” You can ask the assistant to repeat or clarify steps with simple voice commands. This audio-first approach means you stay focused on cooking, and your hands remain free. The narration is context-aware – it references previous steps or your current progress (“Since you only have a small pot, remember we’ll boil the pasta in two batches – I’ll guide you through it.”).

* **Passive Vision Monitoring & Active Feedback:** As you follow the instructions, the camera continuously monitors the scene. The AI checks your actions and the food’s state. If everything looks on track, it stays mostly silent (passive mode). If it detects an issue or something notable, it provides *active feedback*. For instance, if you’re chopping vegetables and the pieces look too large, the assistant might gently say, “Try cutting the carrots a bit thinner for even cooking,” while an AR overlay highlights an example piece that’s oversized. If it sees a pot boiling over, it can alert you to turn down the heat. This kind of real-time feedback – essentially an expert watching over your shoulder – helps prevent common mistakes. The visual analysis is powered by the Gemini 2.5 AI, which can recognize cooking actions and states (like “onions are browning” or “water is boiling”) and even assess doneness by color/texture. All feedback is given in a constructive, non-judgmental tone to keep confidence high.

* **Visual AR Overlays:** The app uses augmented reality overlays on the camera view to direct your attention. Through ARKit, it can draw bounding boxes, arrows, or icons aligned with real objects in your kitchen. For example, when a recipe says “Preheat your oven to 180°C,” the app might highlight the oven in view (if visible) or flash an icon representing an oven. If you need to “add the chopped tomatoes,” the assistant could outline the tomato pile you chopped earlier. During a critical moment like seasoning, it might display a shaker icon over the pot. Warnings or alerts (like a red exclamation) can appear next to something that needs intervention (e.g., a pan that’s too hot). These overlays augment the audio instructions with visual cues, making it crystal clear what to do and where. They are especially helpful for complex tasks like kneading (where to fold the dough) or when multitasking (highlighting the item you should attend to next).

* **Built-in Recipe Library:** The app comes with a curated library of recipes spanning various cuisines and difficulty levels. These are stored in a structured format in a cloud database (Supabase) and include all the metadata needed for guided cooking: ingredient lists, step-by-step instructions, expected timings, required cookware, etc. Users can browse recipes by category (e.g. “Italian Pasta,” “Quick Weeknight Dinners,” “Vegan Dishes”) or search by name or ingredients. Each recipe entry in the library also has an associated image and description, plus tags for dietary restrictions where applicable. Importantly, each recipe in the library is pre-processed: it has been parsed and annotated for the AI, including embeddings of the text. This means the assistant “understands” each recipe’s content, which enables features like semantic search (finding recipes by description or ingredients) and cross-referencing steps or tips from one recipe to another. The built-in recipes serve as a reliable foundation, and more recipes can be continuously added over time (premium subscribers might get new recipes regularly).

* **User-Uploaded Recipe Parsing:** Have a favorite recipe from a blog or an old family PDF? The app lets you import it. You can paste in the text or upload a PDF/image of a recipe, and the assistant will parse it into the structured format it uses for guidance. This is achieved via natural language processing (NLP) powered by the cloud LLM. The parser identifies the list of ingredients (with quantities and units) and the sequence of instructions. It handles common recipe language, converting it into clear, atomic steps. For example, if the text says “Sauté onions until golden,” the system knows “onions” are an ingredient and “sauté until golden” is a step with a visual end condition. It may also detect timers or temperatures mentioned and attach those to the step (e.g., “Bake for 20 minutes at 350°F” becomes a 20-minute timer linked to an oven preheat step). After parsing, you get a chance to review the generated steps and tweak if something looks off, then you can save the recipe to your library. Once saved, the recipe is stored in the Supabase database under your account (with proper relations for ingredients and steps) and an embedding of its content is generated. From then on, you can cook it with full AI guidance just like the built-in recipes.

* **Context-Sensitive Instruction Flow:** The assistant maintains an internal state of the cooking session – knowing exactly which step you’re on, what you’ve done, and what’s next. It uses this context to tailor instructions and timing. If a step takes longer than expected (e.g., water is taking longer to boil), the assistant can fill the time with a helpful tip or reassuring comment instead of simply remaining silent or forcing a skip. If you perform steps out of order (perhaps you pre-chopped something ahead of time), the assistant recognizes that via vision or if you tell it, and it adjusts by skipping or abbreviating those steps in the narration. Crucially, the assistant’s LLM reasoning is used here to make the flow flexible – it’s not just a dumb sequence. For example, if the recipe says to use a stand mixer (which you don’t have, per your profile), the assistant will **adapt** (see the next feature) and alter the flow to a hand-mixing method, possibly splitting one step into a few sub-steps to guide you through the manual process. The context awareness extends to remembering substitutions or user inputs made earlier: if at the start you told the app you’re using tofu instead of chicken, every future reference to “chicken” in instructions will be replaced with “tofu” and it might give tofu-specific cooking tips when appropriate. Overall, the instruction flow feels intelligent and personalized, much like an attentive human chef who knows what you’re doing and what you have.

* **Adaptive Equipment & Ingredient Substitution:** A standout feature of this assistant is its ability to adapt recipes in real time based on the tools and ingredients available. Before and during cooking, the system cross-references the recipe’s requirements with what you have:

  * *Equipment adaptation:* If a recipe calls for a piece of cookware you don’t have (e.g., a large Dutch oven, a blender, or an oven), the assistant will suggest alternative methods or tools. It uses a knowledge base of culinary techniques (accessible via RAG) to fetch instructions on how to achieve similar results with different equipment. For example, if a recipe requires baking in an oven but you only have a stovetop, the assistant might retrieve and instruct a stove-top steaming or pan-baking method (with citations from trusted cooking resources). If you have a smaller pot than recommended, it can suggest cooking in batches or reducing quantities. The guidance adjusts cook times and steps accordingly. This is powered by an internal database of equipment and their substitutions, as well as general cooking science references. The AI essentially asks, “How can we do this step without that tool?” and finds a solution. It then integrates that solution seamlessly into the recipe steps it tells you. This way, lacking a specific gadget never stops you from cooking a recipe – the app finds a workaround.
  * *Ingredient substitution:* Similarly, the assistant can recommend substitutes for ingredients you’re missing or don’t prefer. If you indicate (via voice or in the app) “I don’t have butter,” it might respond, “No butter? I can use olive oil instead for this recipe. The flavor will be slightly different but still good.” It does this by searching a database of ingredient substitutions and flavor profiles. Thanks to the vector embeddings of ingredients in the Supabase database, the app can even find the closest matches in terms of flavor/role – for instance, if out of fresh herbs, it might suggest a dried herb equivalent (with adjusted quantity), or suggest a common substitute like using Greek yogurt in place of sour cream. The substitution is then applied to all future steps of the recipe: instructions will say “oil” instead of “butter”, and the AI will mention any technique changes (like “heat the oil a bit less than you would butter, since it has a lower smoke point”). This feature is not just simple dictionary swaps; it uses AI reasoning and retrieved culinary wisdom to ensure the substitute will work in context (e.g., it won’t suggest something that alters the recipe fundamentally without warning). The goal is to make recipes more flexible – you can still cook a dish even if you’re missing one or two items, or need to swap for dietary reasons.

* **Hands-Free Voice Interaction:** The entire cooking session can be controlled by voice. You can ask questions (“Do I really need to peel the tomatoes?”), request repeats (“Say that again?”), or tell the assistant to move on (“Next step”) without touching the device. The app continuously or intermittently listens for commands using the iOS Speech framework (with user permission). It uses keyword spotting to know when you’re addressing the assistant (for example, you might say a wake word like “Chef” or simply a short silence triggers listening after instructions). Natural language understanding then interprets your request. Many voice commands tie into the AI’s reasoning engine; for instance, asking “Can I use a small pan instead of a wok?” will cause the system to retrieve an answer via its knowledge base and then respond with adaptive instructions. Voice interaction extends to “what-if” questions as well – you can inquire about substituting ingredients or changing portion size on the fly, and the assistant will handle it. All voice replies are given through the same TTS system, maintaining a consistent voice. To avoid false triggers, the app may use on-device processing to ensure it’s actually a command (and not the user talking to someone else or an unrelated sound). A subtle on-screen indicator (and a brief chime) will signal when the assistant is actively listening and when it’s responding.

* **Smart Recipe Recommendations:** Outside of active cooking sessions, the assistant can help you decide *what* to cook. Taking advantage of its ingredient and equipment awareness, it offers recipe suggestions based on what you have on hand. For example, you can show the camera your pantry or verbally list ingredients (“I have chicken, tomatoes, and mushrooms”) and the app will query its recipe database for matches. Using a combination of traditional filtering and semantic search (with Supabase’s hybrid search capability), it finds recipes that either explicitly or conceptually use those ingredients. If no single recipe uses all, it might suggest a couple of options or even a fusion idea (this is where the generative aspect comes in – it can combine knowledge to propose a dish). It also filters based on your equipment: if a recipe usually needs an oven but your profile says you have none, it might prioritize stove-top recipes or ones that have alternative methods. This feature turns the app into a smart meal planner – perfect for when you stare at your fridge wondering what you can make for dinner. It leverages the same AI and RAG pipeline: the query of ingredients/appliances is converted to an embedding, similar recipes or relevant culinary articles (like “10 ways to cook without an oven”) are retrieved, and the LLM formulates a suggestion. Over time, as it learns your taste preferences and logs what you’ve cooked, it can personalize these recommendations even further (e.g., favoring quick recipes on weekdays, or always showing gluten-free options if you ate those in the past).

* **Minimal User Profile (with Cloud Sync via Supabase):** To personalize the experience, the app uses a lightweight user profile stored in a Supabase database. While not mandatory to use the app (a guest mode can use local defaults), creating a profile allows you to save your imported recipes, set your dietary preferences, list available equipment, and sync your recipe history across devices. The profile contains fields like username, email, and some preferences; it’s linked to Supabase’s Auth module for secure sign-in. Importantly, you can record what kitchen devices you own (via a checklist in the app settings) – this informs the equipment adaptation feature. For instance, if you indicate you only have a stovetop and a microwave, the app will automatically filter or adapt recipes that usually require an oven or specialty tools. All recipe data (including your personal recipes) is stored in structured tables on Supabase (see **System Architecture** for schema). Embeddings of recipes, ingredients, and even recipe steps are stored alongside, enabling semantic search and quick retrieval of relevant info. The use of Supabase means the app has a cloud backend that can be updated with new content and knowledge continuously, without requiring frequent app updates. At the same time, privacy controls ensure that only you have access to your personal data – the database entries for your profile and recipes are secured per user. No camera footage is ever sent to the database; it remains transient for AI analysis only.

* **Monetization – Freemium Model:** The app will be free to download with core functionalities available to all users. Free users get access to a selection of recipes (say 50–100 recipes including basic techniques), real-time guidance for those recipes, and the ability to import a limited number of their own recipes. A premium subscription (monthly or annual) unlocks the full library of hundreds of recipes (including new releases every month), advanced AI features, and deeper customization. Premium features might include more complex substitution queries (e.g., adapting recipes to be vegan automatically), integration with smart appliances (if you have any, like smart ovens or thermometers, the app could interface with them to read temperatures or adjust settings), and perhaps the “culinary creativity mode” where the AI can generate entirely new recipe ideas. Subscribers also benefit from higher usage of the AI cloud services (for instance, longer cook sessions or more frequent frame analysis if usage limits are in place to control costs). The app will not serve ads, to maintain an uninterrupted cooking experience – revenue comes from those power users who value the expanded content and capabilities. A one-time purchase option for specific recipe packs (e.g., “Holiday Feast Pack”) may also be offered as an alternative to subscription for those who just want particular content. Regardless of tier, the core promise remains: a reliable AI cooking companion that can adapt to your needs.

The table below summarizes some of the main features and how they’re implemented:

| **Feature**                         | **Description**                                                                                                             | **Implementation Details**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| ----------------------------------- | --------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Ingredient Recognition**          | Identify ingredients in the camera view and highlight them. Helps user confirm they have everything and find items quickly. | *Computer Vision:* Frames from the camera are sent to the cloud vision model (Gemini 2.5) which is fine-tuned for food recognition. Detected ingredients are returned with labels and bounding boxes. The app uses ARKit to place outline overlays and name labels over those items on the screen. If an ingredient expected in the recipe is not seen, the assistant may ask the user to confirm if they have it (or suggest a substitute).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| **Audio Recipe Guidance**           | Step-by-step cooking instructions delivered via voice, adapting to user’s pace.                                             | *Text-to-Speech:* Uses iOS AVSpeechSynthesizer for on-device TTS to minimize latency. The script for each step is either pre-written (from the recipe data) or dynamically generated by the LLM for context (for example, if adapting a step). The audio is queued and timed so that longer steps allow breaks for the user to act. Voice prompts are inserted for user confirmation where needed (e.g., after a major step, “Say ‘next’ when you’re ready to continue”).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| **Vision Feedback & AR Overlays**   | Monitors cooking process and provides visual cues or corrective feedback in real time.                                      | *Vision + AR:* The CV module uses Gemini’s analysis to detect cooking states (color change, boiling, chopping completion, etc.). If a notable event occurs (or doesn’t occur in the expected time), the app triggers an overlay and audio feedback. Overlays are drawn with ARKit’s `ARView` so they stay anchored even if the phone moves slightly. For example, a semi-transparent green checkmark can appear briefly over a bowl when mixing is done correctly. For corrective feedback, a red outline or exclamation mark can highlight the area of concern. The AR content is managed by a SwiftUI interface that interacts with ARKit.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| **Recipe Library (Supabase)**       | Curated set of recipes with structured data and multimedia. Searchable by name, ingredient, etc.                            | *Database:* Stored in Supabase Postgres with tables for recipes, ingredients, recipe\_steps, etc. (see schema). Each recipe has fields like title, cuisine\_type, image\_url, total time, etc., and relations to steps and ingredients. Recipe text and metadata are embedded into a vector representation using OpenAI or Gemini embedding models, stored in a `vector` column (via pgvector). This allows **semantic search** – e.g., a search for “spicy stew with chicken” will find a match even if those exact words aren’t in the title, by comparing embeddings. The app fetches recipes via Supabase’s Swift SDK; most queries can be served offline if cached, but initial search and any content update require internet.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| **Recipe Import & NLP Parsing**     | User can import any recipe (text or PDF) and have it parsed into interactive steps.                                         | *NLP Pipeline:* When a user imports a recipe, the text is sent to an LLM (like GPT-4 via an API) with instructions to output JSON (ingredients list and steps). The system also uses heuristics (like looking for lines that start with numbers or common cooking verbs) to double-check the LLM output. Ingredient names are matched against the app’s ingredients database (to unify naming and possibly get nutritional info or properties). The parsed recipe is then inserted into the Supabase DB under the user’s account – populating the `recipes`, `recipe_steps`, and `recipe_ingredients` tables. Embeddings for the steps and the recipe as a whole are generated and stored, enabling the user to later search their own recipes semantically.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| **Context Awareness & Flexibility** | Remembers past actions, adjusts future instructions, and branches the flow dynamically.                                     | *State Machine + LLM:* The app maintains a state object for the current session (current step index, timers running, substitutions made, etc.). A rule-based state machine (coded in Swift) handles straightforward progress and waiting. For more complex logic, the assistant uses the LLM: it provides the LLM with a summary of the state and any anomalies, and asks what to do. For example, “user has only a small pot, how to adapt boiling pasta step?” – the LLM might respond with a plan to split the pasta and adjust water, drawn from its knowledge or the retrieval of a substitution guide. The app then implements that plan (e.g., adding two sub-steps to the recipe on the fly for batch cooking pasta). This combination of deterministic and AI planning yields a robust yet flexible guidance system.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| **Equipment Adaptation**            | Adjusts instructions if required cookware is unavailable; offers alternative methods.                                       | *User Profile + RAG:* The user profile in Supabase can include a list of owned appliances/tools (or the app can infer some from earlier usage). Each recipe also has a list of required equipment. When a recipe is selected, the app checks for any equipment mismatch. If found, it queries a *Knowledge Base* for alternatives. The Knowledge Base consists of cooking articles and Q\&A (indexed in Supabase with embeddings) about equipment substitutions (e.g., “how to bake without an oven” or “Dutch oven alternatives for braising”). Using RAG, the assistant retrieves relevant passages and the LLM formulates adjusted instructions. Implementation-wise, this happens either at recipe start (providing a general plan: “We’ll use a deep pan instead of an oven for this roast – I’ll guide you through the differences.”) and/or at the specific step requiring the equipment (the instruction for that step is dynamically altered to the alternative method). All such adaptations are done in real time, and the user is informed of any changes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| **Ingredient Substitution**         | Suggests and handles replacements for missing or unwanted ingredients.                                                      | *Ingredient Database + RAG:* The `ingredients` table on Supabase contains an entry for each ingredient, including a vector embedding capturing its culinary context (flavor profile, usage, etc.) and a `properties` array (tags like *dairy, spice, leafy, sweet*). When a user indicates an ingredient is not available (either by scanning the pantry and not detecting it, or user telling via voice “I’m out of X”), the app searches this ingredients table for a similar item. Similarity can be determined by vector distance (semantic similarity) or by matching key properties. Additionally, a curated set of substitution rules (from cookbooks or sites like USDA or Serious Eats) is indexed as well – for example, a document that says “Butter substitute: use 3/4 part oil for 1 part butter in baking, add salt if needed.” The assistant retrieves the best match and confirms with the user: “You don’t have butter. I suggest using olive oil. Shall we do that?” If user agrees, the recipe’s ingredient list is conceptually modified in-memory and all future steps adjust accordingly (the text and speech swap in the word *olive oil*, and any steps about melting butter are changed to heating oil, etc.). The LLM also provides any additional caution or tweak (like the smoke point comment). These substitutions and their sources are stored (perhaps as a log or note) so the user can review later. AI involvement here is critical in making sure the substitute fits the recipe context (e.g., it knows not to replace butter with oil in a cold frosting, because that wouldn’t work). This feature makes the app forgiving – cooking can proceed with what’s on hand, guided by expert knowledge.                                           |
| **Voice Command & Q\&A**            | Full hands-free control and spontaneous Q\&A during cooking.                                                                | *Speech-to-Text + Intent Recognition:* The app uses the iOS Speech framework for continuous or triggered listening. It leverages on-device models for privacy whenever possible. When the user speaks, the transcript is fed into an intent classifier – a lightweight NLP model or rule set distinguishes between navigation commands (“next step”, “repeat step”), queries (“how much salt again?”, “what does sauté mean?”), or contextual statements (“I’ve already mixed that”). Simple commands are handled directly by the app logic (e.g., moving to the next step). Complex questions are forwarded to the AI reasoning module: the LLM gets the question plus relevant context (current step, possibly an image snapshot if relevant, and retrieved info from the knowledge base if needed). It then formulates a concise answer. For example, if asked “What can I use if I don’t have a whisk?”, the system will retrieve a known substitution (like “use a fork or shake ingredients in a jar” from a cooking tips document) and answer accordingly. The voice output is then spoken to the user. Ensuring low latency is important; the system might use a partial recognition (end-of-speech detection) to start fetching an answer even before the user finishes speaking, to make it feel snappy.                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| **Supabase Cloud Backend**          | Central storage of content and user data; vector-enhanced for AI queries.                                                   | *Supabase (PostgreSQL + pgvector):* The app’s back end is built on Supabase, which provides a hosted Postgres database with an integrated authentication and file storage. The relevant tables in the database (simplified from the schema) include: **profiles** (user info/preferences), **recipes** (recipe metadata, including a vector embedding of the recipe description for semantic search), **recipe\_steps** (each step text, linked to a recipe, optionally with an embedding for advanced search or context retrieval), **ingredients** (master list of ingredients with name, properties, and embedding vector for similarity queries), and **recipe\_ingredients** (the join table linking recipes to their ingredients and quantities). This normalized schema allows efficient queries like “find all recipes that use ingredient X” or “get all steps for recipe Y in order.” The use of vector columns (via the `VECTOR` data type from pgvector) enables semantic search and RAG: for instance, when the LLM needs to answer a question, it can perform a similarity search in the `recipe_steps` table to find if any step in any recipe is relevant, or search the `ingredients` table to find related ingredients. Supabase’s API and client library allow the app to query these with ease (e.g., using a SQL query or RPC to perform an embedding similarity search server-side). In practice, this means the knowledge base for the AI is partly stored in the same database as the recipes, making data management simpler and more secure (no third-party vector DB needed). All user-specific data (like imported recipes) are isolated by user\_id, while shared data (the curated recipe library and general cooking references) are readable by all. |

## System Architecture

The system consists of the iOS client application, cloud AI services (for vision and language), and the Supabase backend. The diagram below outlines key components and data flow:

```
iPhone (Client App)                           
├─ Camera & AR View (ARKit)        - Captures live video frames (≈1 FPS) for analysis 
│                                   and displays AR overlays on screen.
├─ Voice Interface (Mic/Speech)    - Listens for user voice commands or questions, 
│                                   sends audio to STT, and receives text.
├─ Cooking Session Manager         - Core logic orchestrating recipe steps, state, 
│                                   and coordinating other modules.
│   ├─ Recipe State Machine        - Tracks current step, timers, completed actions.
│   ├─ AI Reasoning Engine         - Calls LLM for contextual decisions, explanations, 
│   │                               or dynamic instruction generation.
│   ├─ Vision Analysis Client      - Sends frames to cloud vision model, receives 
│   │                               ingredient/action detections and scene info.
│   ├─ RAG Query Builder           - Formulates queries for knowledge retrieval (including 
│   │                               ingredient/equipment substitution queries).
│   ├─ Supabase Data Client        - Queries the recipe/ingredient database (incl. vector search) 
│   │                               for relevant info (e.g., substitution suggestions, user’s own data).
│   └─ Response Composer           - Gathers outputs (LLM text, retrieved info, state updates) 
│                                   to produce final user-facing responses (audio speech + overlays).
├─ Audio Output (TTS)              - Speaks the instructions or answers using AVSpeechSynthesizer.
└─ UI Layer (SwiftUI)              - Displays recipe info, listens status, minimal controls, and 
                                    renders AR overlays via ARKit integration.
                                    
Cloud Services:
├─ **Gemini 2.5 Pro (Multimodal LLM)** - Processes visual frames (image input) and textual prompts for reasoning.
│   - Vision API: Receives an image (frame) and returns detected objects (ingredients, utensils), 
│                actions (e.g. "chopping", "stirring"), and scene descriptors (e.g. "onions are getting brown").
│   - Chat/Completion API: Receives conversational prompts (including retrieved documents, context, and questions) 
│                          and returns answers or next-step instructions. This is the core reasoning for the assistant.
│   - Embedding API: Converts text (e.g., recipe text, user queries) into high-dimensional vectors for similarity search.
└─ **Supabase Backend (Postgres)**    - Stores structured data and acts as vector database.
    - Auth: Handles user sign-up/login and secure access (each session gets an access token).
    - Database: Tables for recipes, ingredients, steps, etc., with pgvector for embeddings.
    - Edge Functions (if used): Could host custom logic like calling the OpenAI API for embeddings, or performing RAG queries if not done client-side.
```

Here’s a step-by-step walkthrough of how these components interact during a typical recipe session, especially highlighting the new adaptive features:

1. **Session Initialization:** When the user selects a recipe and hits “Start Cooking,” the app fetches the recipe details (steps, ingredients, required equipment) from Supabase. The **Cooking Session Manager** is initialized with this data. It also pulls the user’s profile (to know preferences and what equipment they have). For example, the recipe might require an oven and a blender. If the profile indicates the user has no oven but has a stove, the manager flags this. It will later trigger an adaptation when those steps come. If ingredients are listed, the app might immediately use the camera to scan the counter for them. The Vision Analysis Client sends a frame to Gemini – which returns, say, that it sees “tomatoes, onion, and a pot on the stove.” The app compares that to the recipe’s ingredient list and might notice that “garlic” was not identified. It asks the user via voice, “I don’t see the garlic – do you have any? If not, I can suggest a substitute.” The user could then say, “I ran out of garlic,” and the system will engage the substitution feature (we’ll see that in a moment). After this quick inventory check, the app is ready to begin.

2. **Step Execution – Guidance and Monitoring:** The state machine sets the current step to 1 and the Audio Output narrates the first instruction (e.g., “Step 1: Chop the onions and garlic.”). The user proceeds to chop. The Vision module gets a frame each second, detecting the action (“chopping”) and objects (“onion pieces”, “knife”). As the user chops, the AI Reasoning Engine receives these observations. If the user had said they have no garlic, by now the RAG Query Builder would have fetched a substitute suggestion (perhaps “use half a teaspoon of garlic powder for each clove” from a reference). The LLM would have modified the instruction accordingly: it might have actually told the user “Chop the onions. Since you don’t have fresh garlic, we’ll add a bit of garlic powder later instead.” Assume the user is chopping just onions now. The vision might notice the user’s chopping technique or the size of pieces. If pieces are uneven, the system might retrieve a quick tip from its knowledge base (maybe a knife skills snippet) and the LLM forms a gentle suggestion. The Response Composer then interrupts with, “Try to keep the onion pieces about the same size. It helps them cook evenly.” (This message is short so as not to overwhelm, and timed while the user is still chopping.)

3. **Adaptive Equipment Use:** Now say Step 2 in the recipe was “Preheat the oven to 180°C.” The app knows the user doesn’t have an oven. Instead of this standard instruction, the Session Manager consults the AI Reasoning Engine for an alternative. The Query Builder creates a query like: *“How to roast or bake \[dish] without an oven, using a stovetop?”* along with context (it knows what we’re cooking – e.g., a roast chicken or a casserole – and that a stovetop and pot are available). It searches Supabase’s knowledge base embeddings and finds a relevant passage (for instance, an article about stove-top roasting or using a Dutch oven on stove as an oven substitute). These passages are fed into Gemini’s chat model with a prompt about rewriting the recipe step. Gemini might return an adjusted plan: “Instead of oven: use a large deep pan on the stovetop. Preheat it on medium with a lid on (to mimic oven heat). Then place the food in and cover, checking occasionally.” The Session Manager takes this and splits the original Step 2 into a custom sequence for the user:

   * *New Step 2:* “Heat a deep pan on medium heat with a lid on – we’ll use it as an oven substitute.”
   * *New Step 3:* “Once heated, place the chicken in the pan and cover it. We’ll cook for about X minutes (instead of oven 20 minutes), flipping halfway.”
     These dynamic steps are spoken to the user when we get there, and the overlay might highlight the stove instead of an oven. The user can thus continue without an oven, guided properly. Under the hood, the adaptation logic leverages both the knowledge base and the real-time LLM to ensure the instructions are coherent and safe.

4. **Ingredient Substitution in Action:** Let’s illustrate an ingredient swap. Suppose the recipe calls for **butter** to sauté vegetables in Step 3, but earlier the user indicated they have no butter (or the camera never saw butter and user confirmed none). Prior to reaching Step 3, the RAG system already looked up a butter substitution. The ingredients database knows butter is a fat with certain properties; a similarity search might have pulled up “olive oil” as a close match (embedding-wise) and also a text snippet from a cooking FAQ that says “If you don’t have butter, you can use olive oil or vegetable oil in most sautéing, use 3/4 of the amount of butter”. The assistant chooses olive oil (perhaps considering the recipe’s cuisine and what’s available). So when narrating Step 3, it will say: “Step 3: Add 2 tablespoons of olive oil to the pan (we’re using olive oil instead of butter here) and let it heat up.” The AR overlay might show an oil bottle icon rather than a butter icon. The session manager also adjusts any internal variables (like later if the recipe said “add remaining butter to sauce,” it knows to interpret that as oil). The user follows along, using oil. If needed, the assistant might add a note about the difference (“Olive oil burns faster than butter, so keep an eye on the heat.”). All of this is delivered seamlessly as part of the normal instructions – the user doesn’t have to do anything special; the assistant proactively handled the substitution once it was confirmed.

5. **Integration with Knowledge Base (RAG) for Q\&A:** During cooking, the user might have ad-hoc questions. For instance, while the vegetables are sautéing, the user asks: “Can I use the same pan to cook the sauce later?” The app’s speech recognizer transcribes this and sends it to the AI Reasoning Engine. The Query Builder sees this is a question and not directly answered by the recipe text. It formulates a query, perhaps combining context (we’re making a certain sauce, the pan currently has sautéed veggies) and searches any general cooking knowledge for “reusing pan for sauce after sauté”. It might find a generic advice snippet: “It’s usually fine to use the same pan for the sauce to save flavor, as long as the pan isn’t burnt.” The LLM then answers: “Yes – in fact, using the same pan can add extra flavor. Just make sure there’s no burnt residue before you start the sauce.” The Response Composer sends this to Audio, and the user hears the helpful tip immediately. This all happens within a second or two, thanks to efficient vector search on Supabase and fast generative response. If the question had been about something like “Why do I need to cook in batches in a small pot?”, the assistant would similarly retrieve an explanation (like a science article on overcrowding pans lowering temperature) and explain that to the user in approachable terms.

6. **Multimodal Checks and IoT Integration:** (Future/advanced example) If the user has certain smart devices, the app could integrate – for example, reading a smart thermometer in meat to decide when it’s done. But without digressing, even with just camera vision, the assistant can do a lot. Let’s say near the end, the recipe says “Simmer until the sauce thickens.” The vision module looks at the sauce each frame, trying to gauge thickness (perhaps by how it drips from a spoon the user lifts, etc.). This is complex, but the assistant at least knows an approximate time from the recipe. It sets a 5-minute timer in the app. During this waiting time, it might engage the user with a relevant tip or even a story (some premium content or a kitchen trivia from the knowledge base). Once it detects via vision or timer that the sauce is thick (or 5 minutes passed), it alerts, “The sauce looks thick now.” and moves to plating instructions.

7. **Completion and Save Outcome:** After finishing the last step, the assistant congratulates the user: “All done! Great job, your dish is ready. Enjoy your meal!” On the UI, the session ends and a summary screen appears. Because we have a cloud backend, the app now records this session: it logs the recipe, date/time, any substitutions made, and possibly asks the user for a rating or notes. The user might type or say “That was tasty but a bit spicy.” This could be saved in a `notes` field associated with that recipe-user combination in the DB. The history (maybe accessible in a History tab) helps the user remember what they’ve cooked and liked. It also feeds into the recommendation engine – e.g., if they cooked a lot of Italian dishes, the app might suggest more of those. The end of session also might prompt: “Would you like to share your success or save a photo?” (Social features could be future enhancement, not core now). Finally, the app ensures any sensitive data in memory is cleared and camera/mic are turned off.

Throughout this flow, latency is managed carefully. The 1 FPS frame rate is enough for making decisions at human cooking speed. Voice commands are typically in moments when the user is waiting or doing a steady task, so a 1-2 second response time is acceptable. The architecture uses background threads for network calls (so the UI/rendering never stutters). We leverage Supabase’s real-time capabilities where appropriate – for example, if new recipes or tips are added to the knowledge base, they can sync to the device in the background. This means the assistant actually “learns” new substitutions or techniques over time without app updates, simply by the knowledge base growing (for instance, if a new popular ingredient or an alternative cooking method emerges, adding it to the DB makes it available to the AI immediately).

## UI/UX Design Considerations

The user interface is designed to be as unobtrusive as possible during cooking, given that the primary interaction is via voice and AR visuals on the camera feed. Key points:

* **Pre-Cooking UI:** The recipe browsing interface is a typical modern iOS app style – a scrollable list of recipes with search and filters. Each recipe card might show an image, title, and maybe icons for difficulty, time, and required equipment (so users can quickly see if they need an oven, blender, etc.). If the user opens a recipe, they see a detail view with ingredients and tools required, and a “Start Cooking” button. On this screen, if there are potential mismatches (like the recipe requires an oven and the user’s profile says none), we might show a small notice: “Needs Oven – we will suggest a stove-top method” so the user isn’t caught off guard. The user can also edit which ingredients they have or want to substitute on this screen (tapping an ingredient brings up alternatives). However, all this can also be handled dynamically during the session.

* **Cooking Session UI:** Once started, the app likely goes into a landscape orientation (if that provides a wider view for the camera) or stays portrait but with a full-screen camera view. AR overlays will appear on this camera feed to augment the real-world view. We keep other UI elements minimal: perhaps a small overlay showing the current step number and name (for reference if one does glance), a waveform or indicator when the app is listening for voice, and basic controls like pause or end session (in case of emergency or user needs to stop). These controls could be semi-transparent buttons at the edge. The text of the current instruction might be accessible (some users might like to read it) – possibly by tapping a “Transcript” button that toggles a sidebar with the written instructions. However, by default that’s hidden to keep the focus on the food itself. The AR overlays use color coding (green for confirmations, yellow for tips, red for warnings) and simple iconography that’s easy to understand at a glance.

* **Voice and Feedback UX:** Because the assistant speaks, we avoid the need for the user to read screens, but we also must ensure the user knows the assistant heard them. So, after a voice command, the app will immediately give a subtle audio/visual acknowledgment. For example, user says “Next step” – the app might play a quick **ding** and maybe the assistant voice says “OK” or just directly starts the next instruction. If the user asks a question, the app might respond with a brief pause (“Hmm…”) as it searches, to signal it’s working, then answer. We also plan for misunderstandings: if the speech recognizer isn’t confident or the command was unclear, the assistant will ask for clarification: “Sorry, I didn’t catch that. Could you say it again?” This should be infrequent as the domain is fairly constrained.

* **Adaptation Transparency:** When the assistant adapts a recipe (equipment or ingredient), it will explicitly inform the user, but in a positive way. For instance, “Don’t worry, we can still make this dish without an oven. We’ll use a stove-top method I know.” This builds trust that the AI is handling the deviation. If a substitution is made, it will confirm (as noted, like asking if using olive oil is okay). We want to avoid a scenario where the user is confused why the instructions don’t match the original recipe they know – so we always mention the change briefly. Perhaps even on the UI, a small tag like “Adapted for no-oven” could be shown.

* **Supabase Sync and Offline:** From a UX perspective, the user may log in to save data, but we don’t force login just to use the app. A first-time user can click “Continue as Guest” and try a demo recipe immediately – in that case, a default profile with standard equipment (assuming a basic kitchen) and no dietary restrictions is used. If they want to import recipes or have a history, they’ll be prompted to create a free account. Syncing is largely behind the scenes; if they log in on a new device, their saved recipes and preferences come down via Supabase automatically. If the device is offline, the app will warn that the full AI features are unavailable and default to a limited mode (preloaded recipes, no vision or Q\&A). We clearly communicate that on the start screen (“Offline Mode: Vision and Q\&A disabled”).

* **Visual Design:** The app’s color scheme and typography will be chosen for clarity and a modern feel. Important text (like ingredient names in overlays) will have high contrast outlines so they’re visible against real world backgrounds. We may use a friendly mascot or character to represent the AI (perhaps a small animated chef’s hat icon that bounces when speaking or listening) – something to give the assistant a bit of personality without being distracting. Animations will be subtle and purposeful (like a gentle highlight pulsing on an item to draw attention).

* **Error Handling UX:** If something goes wrong (e.g., vision fails to analyze for a while due to poor lighting, or the Supabase queries aren’t returning), the app will not crash or stop; instead, it will gracefully fallback. For instance, if the camera can’t identify items, the assistant might switch to asking the user more questions (“Let me know when you’ve added the onions.”). If a cloud service is down mid-cooking, the assistant has some baseline knowledge (from the recipe data and any cached tips) to continue on script if possible, and apologizes for reduced capabilities. We aim to always provide *some* guidance rather than leaving the user stranded.

## Technical Challenges and Mitigations

Designing this AI cooking assistant involves addressing several challenges:

* **Real-Time Performance:** Handling vision analysis, voice recognition, database queries, and LLM calls in near-real-time on mobile hardware is non-trivial. We mitigate this by (a) using cloud services for heavy tasks (Gemini for vision and language) and streaming results as they come, (b) limiting vision to \~1 FPS and resolution to what’s needed (e.g., 720p frames, compressed) to reduce bandwidth and latency, and (c) pre-fetching and caching data where possible. For example, while the user is on step 1 chopping, we can already fetch embeddings for step 2’s context (like equipment instructions) so that if a query is needed, it’s faster. Supabase’s vector search is very efficient (HNSW index in Postgres) and co-located with our data, so retrieval queries come back in tens of milliseconds typically. We also make liberal use of background threads and OperationQueues in iOS to ensure the UI thread is free. In testing, we’ll monitor frame processing times and might adjust the frequency dynamically (if the app detects it’s running on an older iPhone, it might do vision at 0.5 FPS to save CPU and battery).

* **AI Hallucinations and Safety:** Using a powerful LLM means sometimes it might generate instructions or answers that sound confident but are incorrect or even unsafe (e.g., forgetting that a substitute won’t work, or giving a wrong cooking time). To guard against this, we rely on *retrieval augmentation* heavily for any factual or technical question. The LLM is instructed to base its answers on retrieved sources (which we curate for reliability, like serious cooking sites or known substitution tables). For critical steps (like food safety-related instructions: cooking meat to safe temperature), we either hard-code minimums or provide the LLM with USDA guidelines from the knowledge base so it doesn’t guess. We also use a moderation layer: after the LLM produces an instruction, the app can quickly scan it for certain unsafe keywords (like “raw chicken okay” which should never appear). Because we update the knowledge base, we can quickly fix any content issues by editing or adding better references rather than needing to retrain the model.

* **Adapting Recipes Correctly:** Changing equipment or ingredients in a recipe can fundamentally alter the outcome. It’s a challenge to ensure the assistant’s adaptation still leads to a successful dish. We approach this by starting with simple, well-known substitutions (the low-hanging fruit of equivalences). The knowledge base includes tested advice: e.g., *“to simulate an oven, you can use a large covered pot (a Dutch oven) on stove”*, or *“replace each egg with 1/4 cup applesauce in baking”* for vegans. The AI defers to this data. If a user asks for an unusual substitution (something outside the knowledge base), the app might respond with a cautious note (“I’m not sure that will work well, but you could try…”). Essentially, we program the assistant to *know what it doesn’t know*. Technically, we can have the LLM emit a confidence or source usage indicator (via its function calling or a special token) and if it’s below threshold, we don’t act on it without user confirmation. Rigorous testing with various substitutions will be needed – we might even restrict some: e.g., if a key ingredient is missing (like trying to make meringue without any aquafaba or egg white substitute), the assistant should honestly say the recipe can’t be made correctly, and perhaps suggest a different recipe that uses what is available.

* **User Privacy:** The camera is effectively watching the user’s kitchen, and the microphone is listening – users might feel uneasy. We address privacy on multiple levels. First, all image analysis is in memory or via encrypted channels to the AI; we do not record or store any video. We will be transparent about this in onboarding (“Camera images are processed in real-time to guide you, and never saved or sent to any server except for AI analysis, with no identifying info”). For voice, similarly, the raw audio is not stored, only the text of recognized commands might be logged (and even that can be turned off in settings). The Supabase backend stores recipe data but not anything like full video or audio transcripts of a session (unless the user explicitly records a session which is not a feature in our scope). All personal data follows GDPR best practices (user can request deletion, etc.). By building trust (the app works well and clearly respects privacy), users will be more likely to opt in to the full experience.

* **Supabase and Sync Issues:** Relying on a cloud database means the app must handle network disruptions gracefully. We use Supabase’s offline capabilities where possible (it has local caching for certain queries). If a user starts a session with a recipe already fetched, we ensure all needed info is cached on device, so even if connectivity drops mid-cook, the basic guidance can continue (the AI cloud features would drop, but at least the step texts are there). If Supabase itself is down (rare, but possible), the app can fallback to a read-only local copy of the core library (maybe shipped with the app or cached from last use). These contingencies keep the app functional in various conditions, albeit with reduced functionality.

* **Scalability and Cost:** The design uses heavy AI processing which can be costly (especially the vision and LLM API calls). To scale to many users, we’ll implement usage optimizations: e.g., not every single frame needs to be analyzed if the scene hasn’t changed (we can do basic motion detection on device to decide if a new frame should trigger analysis). We can also adjust quality: the LLM can be called with smaller models or lower precision for less critical tasks. As user volume grows, the Supabase vector search might need to handle many requests – we ensure proper indexing and perhaps use read replicas or caching for popular queries (like common substitution questions). We will also likely enforce some limits on free users (e.g., number of AI queries per session) to manage costs, while ensuring the basic guided cooking (which can largely rely on pre-authored recipe content) remains free. The modular architecture (client does orchestration, cloud does AI heavy lifting) allows us to update or swap out AI models as more efficient ones become available (for instance, if a smaller open-source model can run on device in the future, we could shift to that to reduce cloud calls).

## Conclusion

The iOS AI Cooking Assistant is an ambitious fusion of augmented reality, real-time AI, and culinary knowledge aimed at revolutionizing the home cooking experience. By **adapting to the user’s kitchen – both in terms of ingredients and equipment – and providing expert guidance every step of the way**, it lowers the barrier to trying new recipes and techniques. The product design leverages an iOS-centric tech stack (SwiftUI, ARKit, AVFoundation) on the front end and powerful cloud components (Gemini multimodal AI and a Supabase Postgres vector database) on the back end. This combination allows the assistant to “see” and “understand” what’s happening in the kitchen and respond with helpful, tailored instructions in real time.

We have designed for **flexibility** (through RAG-driven adaptations and user personalization), **usability** (voice-first interface and clear AR visuals), and **reliability** (robust handling of errors and context awareness). Importantly, the system is built to learn and improve: as more users cook and ask questions, the knowledge base can expand with more solutions, and the AI will become even better at handling edge cases (like unusual substitutions or creative cooking methods). In essence, every cooking session is not only a service to the user but also an opportunity for the assistant to get smarter (within the limits of privacy – learning in aggregate, not storing personal footage).

From a business perspective, focusing on iOS lets us deliver a high-quality experience to a tech-savvy audience who are likely to be early adopters and subscribers. The Supabase backend gives us a scalable foundation for user accounts, data, and the AI knowledge store without needing to build a custom server from scratch, accelerating development and iteration.

In conclusion, the AI Cooking Assistant stands to become a pioneering product in smart kitchens. It goes beyond static recipe apps by actively engaging with the user and environment. Whether someone is a complete beginner needing hand-holding through a simple recipe or an experienced cook looking to expand their repertoire (and make do with ingredient substitutions), the assistant rises to the occasion. With this design, we’re not just digitizing recipes – we’re recreating the experience of having a personal chef mentor in your kitchen, one that knows exactly what you have and how to help you use it best. The result will be more people cooking confidently at home, wasting less (“oh, I can use X since I’m out of Y!”), and enjoying the process as much as the result.

**Sources:**

1. Onix-Systems Blog – *Top AI Cooking Assistant Use Cases* (2023), highlighting features like ingredient substitutions and appliance-based time adjustments.
2. Supabase Documentation – *AI & Vectors: Hybrid Search* (2023), illustrating how embeddings are stored in Postgres for semantic search.
3. Postgres pgvector Tutorial – Example of using a `VECTOR` column and HNSW index for fast similarity queries in a recipe context.
4. InData Labs – *AI Cooking Assistant Case Study* (2022), on voice-controlled recipe guidance and NLP parsing of recipes (for import feature).
5. Vir, R. & Madinei, P. – *ARChef: AR Cooking Assistant Powered by Multimodal AI* (2024), which inspired our AR overlay approach for ingredient identification and step guidance.
6. Nguyen et al. – *Interactive Cooking Guide with Augmented Reality* (IEEE, 2022) – research on using AR and voice for guided cooking, informing our hands-free UX design.